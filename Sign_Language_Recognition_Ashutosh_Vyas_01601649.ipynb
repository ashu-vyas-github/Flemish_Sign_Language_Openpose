{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules\n",
    "\n",
    "Note: For the successful loading of all the modules, it is necessary that LightGBM is installed in the current python environment.\n",
    "\n",
    "The assumed directory structure is,\n",
    "1. Current dir: ../\n",
    "2. Data dir: ../data/ contains pose/train, pose/test/, labels.csv, files.txt\n",
    "3. Code dir: ../code/\n",
    "4. Execution file dir: ../code/Sign_Language_Recognition_Ashutosh_Vyas_01601649.ipynb\n",
    "5. Utilities dir: ../code/util/ contains helpers.py, vis.py, straified_groupk.py, results_plots_evalutaion.py\n",
    "6. Feature processing dir: ../code/feature_engineering contains data_augmentation.py, feature_extractors_4D_array.py, feature_preprocessing.py, features_4D_array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "import util.vis as V\n",
    "import util.helpers as H\n",
    "import data_analysis\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import gc\n",
    "from glob import glob\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import feature_engineering.feature_preprocessing as feat_prepro\n",
    "import feature_engineering.feature_extractors_4D_array as feat_extract\n",
    "from feature_engineering.data_augmentation import SLRImbAugmentation\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GroupKFold\n",
    "from util.stratified_group_cv import StratifiedGroupKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from util.results_plots_evaluation import map3_scorer\n",
    "import util.results_plots_evaluation as results\n",
    "from sklearn.metrics import accuracy_score\n",
    "import util.helpers as kaggle_submission\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Imports done...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize feature extraction flags and data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='raise', divide='raise', over='raise', under='raise', invalid='raise')\n",
    "rng = np.random.RandomState(42)\n",
    "startTime= datetime.now()\n",
    "\n",
    "# #splits for cross-validation and #frames for interpolation\n",
    "n_splits = 5\n",
    "interpolated_total_frames = 15\n",
    "\n",
    "# Initialize features extraction flags\n",
    "face_flag = True #False for feature set 1\n",
    "body_flag = True #False\n",
    "hand_flag = True #False\n",
    "physics_flag = True\n",
    "trajectory_flag = True #False\n",
    "linear_flag = True\n",
    "angular_flag = False\n",
    "std_flag = False\n",
    "velocity_flag = False\n",
    "acceleration_flag = False\n",
    "remove_keypoints = True #False\n",
    "save_plot = False\n",
    "\n",
    "# 137 keypoint indices to remove if remove_keypoints is True\n",
    "# current list is for lower-body and many face keypoints\n",
    "unwanted_keypoints=[10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n",
    "\n",
    "# Paths to load the data\n",
    "DATA_DIR = '../data'\n",
    "POSE_DIR = '../data/pose'\n",
    "TRAIN_DIR = POSE_DIR + \"/train\"\n",
    "TEST_DIR = POSE_DIR + \"/test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read labels.csv into a pandas dataframe for convenience\n",
    "full_dataframe = pd.read_csv(pjoin(DATA_DIR, \"labels.csv\"))\n",
    "full_dataframe['Data'] = full_dataframe['File'].apply(lambda title: np.load(pjoin(TRAIN_DIR, title + \".npy\")))\n",
    "print(full_dataframe.head())\n",
    "\n",
    "# 4D data as (n_samples, n_frames, n_keypoints, n_coords)\n",
    "samples_centered_4D_array = feat_prepro.interpolate_allsamples(full_dataframe.Data, interpolated_total_frames=interpolated_total_frames, x_resolution=1.0, y_resolution=1.0)\n",
    "\n",
    "print(\"\\nInterpolated training data shape\",samples_centered_4D_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split if necessary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Convert to code block if needed. The split is non-stratified.\n",
    "# Train and test split required only for Confusion Matrix or if comparing test scores with Kaggle\n",
    "X_train, X_valid, y_train, y_valid, group_train, group_valid = train_test_split(samples_centered_4D_array, np.asarray(full_dataframe.Label), np.asarray(full_dataframe.Person), test_size=0.25, random_state=42, shuffle=True, stratify=None)#=np.asarray(full_dataframe.Label))\n",
    "\n",
    "print(\"Training shape 4D split\",X_train.shape)\n",
    "print(\"Validation shape 4D split\",X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Initialize the augmentaion class, convert to code block\n",
    "slr_obj = SLRImbAugmentation()\n",
    "aug_samp, aug_label, aug_group = slr_obj.fit(X=X_train, y=y_train, groups=group_train, augmentation_factor=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "\n",
    "Extract features as per above enabled flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use this block if just performing cross-validation on original data\n",
    "### NOT FOR Train-test split and augmentation, see next block for that.\n",
    "print(\"\\nExtracting features for training data\")\n",
    "X_train = feat_extract.main_feature_extractor(array_4D_data=samples_centered_4D_array, face=face_flag, body=body_flag, hands=hand_flag, physics=physics_flag, trajectory=trajectory_flag, linear_flag=linear_flag, angular_flag=angular_flag, std_flag=std_flag, velocity_flag=velocity_flag, acceleration_flag=acceleration_flag, remove_keypoints=remove_keypoints, unwanted_keypoints=unwanted_keypoints)\n",
    "\n",
    "y_train = np.asarray(full_dataframe.Label)\n",
    "group_train = np.asarray(full_dataframe.Person)\n",
    "print(\"Training shape\",X_train.shape)\n",
    "print(\"NAN values:\",np.isnan(X_train).sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Convert this to code block for using with augmentation\n",
    "print(\"\\nExtracting features for training data\")\n",
    "X_train = feat_extract.main_feature_extractor(array_4D_data=aug_samp, face=face_flag, body=body_flag, hands=hand_flag, physics=physics_flag, trajectory=trajectory_flag, linear_flag=linear_flag, angular_flag=angular_flag, std_flag=std_flag, velocity_flag=velocity_flag, acceleration_flag=acceleration_flag, remove_keypoints=remove_keypoints, unwanted_keypoints=unwanted_keypoints)\n",
    "\n",
    "y_train = aug_label\n",
    "group_train = aug_group\n",
    "print(\"Training shape with augmentation\",X_train.shape)\n",
    "print(\"NAN values:\",np.isnan(X_train).sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### To be used with train-test split enabled, convert to code block\n",
    "\n",
    "print(\"\\nExtracting features for validation data\")\n",
    "X_valid = feat_extract.main_feature_extractor(array_4D_data=X_valid, face=face_flag, body=body_flag, hands=hand_flag, physics=physics_flag, trajectory=trajectory_flag, linear_flag=linear_flag, angular_flag=angular_flag, std_flag=std_flag, velocity_flag=velocity_flag, acceleration_flag=acceleration_flag, remove_keypoints=remove_keypoints, unwanted_keypoints=unwanted_keypoints)\n",
    "print(\"Validation shape\",X_valid.shape)\n",
    "print(\"NAN values:\",np.isnan(X_valid).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize objects for Scaling, PCA, Cross-validation, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard Scaler\n",
    "scl = StandardScaler()\n",
    "# scl = RobustScaler()\n",
    "\n",
    "### PCA\n",
    "# pca_obj = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "### Cross validator\n",
    "# cvld = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, train_size=None, random_state=42)\n",
    "# cvld = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "cvld = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "### Feature Selector\n",
    "feature_selector = VarianceThreshold(threshold=0.0)\n",
    "# feature_selector = SelectKBest(k=int(0.5*X_train.shape[1]))\n",
    "\n",
    "### Flush the RAM before training\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize one estimator\n",
    "\n",
    "Note: The hyper-parameters are set and initialized as per the GridSearchCV tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Estimator\n",
    "### Enable and select only one at a time\n",
    "\n",
    "# estimator = LogisticRegression(C=0.275, tol=1e-4, max_iter=5000, penalty='l2', class_weight=None, multi_class='ovr', random_state=42, n_jobs=-1)\n",
    "\n",
    "# estimator = SVC(C=6.5, decision_function_shape='ovo', kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, break_ties=False, random_state=42)\n",
    "\n",
    "# estimator = SVC(C=6.5, decision_function_shape='ovr', kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, break_ties=False, random_state=42)\n",
    "\n",
    "# estimator = SVC(C=0.0775, decision_function_shape='ovo', kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, break_ties=False, random_state=42)\n",
    "\n",
    "# estimator = SVC(C=0.0775, decision_function_shape='ovr', kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, break_ties=False, random_state=42)\n",
    "\n",
    "# estimator = RandomForestClassifier(n_estimators=130, max_depth=16, min_samples_split=2, min_samples_leaf=1, max_features='auto', max_leaf_nodes=150, bootstrap=True, criterion='entropy', min_weight_fraction_leaf=0.0, min_impurity_decrease=0.0, min_impurity_split=None, oob_score=True, n_jobs=-1, random_state=42, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "# estimator = LGBMClassifier(boosting_type='gbdt', num_leaves=15, max_depth=15, learning_rate=0.525, n_estimators=102, objective='multiclass', min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, importance_type='gain', subsample_for_bin=200000, class_weight=None, random_state=42, n_jobs=-1, silent=True)\n",
    "\n",
    "# estimator =  GaussianNB(priors=None, var_smoothing=110)\n",
    "\n",
    "########### Below are the estimators for body+mean features\n",
    "estimator = LogisticRegression(C=0.9, tol=1e-4, max_iter=5000, penalty='l2', class_weight=None, multi_class='ovr', random_state=42, n_jobs=-1)\n",
    "# estimator = LGBMClassifier(boosting_type='gbdt', num_leaves=18, max_depth=5, learning_rate=0.525, n_estimators=170, objective='multiclass', min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, importance_type='gain', subsample_for_bin=200000, class_weight=None, random_state=42, n_jobs=-1, silent=True)\n",
    "\n",
    "print(\"\\nTraining the model\", str(estimator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the pipeline object with above configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('scale', scl), ('reduce_dims', pca_obj), ('clf', estimator)])\n",
    "pipe = Pipeline([('selection', feature_selector), ('scale', scl), ('clf', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and perform hyperparameter tuning with grid search\n",
    "\n",
    "Note: Currently GridSearchCV object is initialized for Logistic Regression. Refer Appendix at the end of this file for parameter grid for other estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid Search CV\n",
    "param_grid = dict(clf__C=[0.85, 0.9, 0.95]) # Refer Appendix\n",
    "\n",
    "print(\"Running GSCV.....\")\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=cvld, n_jobs=-1, verbose=100, scoring=map3_scorer)\n",
    "grid.fit(X_train, y_train, groups=group_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "pipe_submission = grid.best_estimator_\n",
    "\n",
    "map3_trn, map3_vld = results.predict_print_results(pipe_submission, X_train, X_train, y_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Enable this only if generating learning curve, validation curve, confusion matrix plots\n",
    "\n",
    "### Learning Curves\n",
    "print(\"\\nPlotting Learning Curve.....\")\n",
    "title = str(estimator)\n",
    "results.plot_learning_curve(pipe_submission, X_train, y_train, groups=group_train, title=title, shuffle=True, ylim=(0.0, 1.1), cv=cvld, n_jobs=-1, save_plot=save_plot)\n",
    "\n",
    "### Validation Curves\n",
    "print(\"\\nPlotting Validation Curve.....\")\n",
    "param_name = 'estim__C'\n",
    "param_range = np.logspace(-2,1,4) #pipe_submission.set_params(estim__C=)\n",
    "results.plot_validation_curve(pipe_submission, X_train, y_train, param_name=param_name, param_range=param_range, ylim=(0.0, 1.1), groups=group_train, cv=cvld, xlog=False, save_plot=save_plot)\n",
    "\n",
    "### Confusion Matrices\n",
    "print(\"\\nPlotting Confusion Matrix.....\")\n",
    "results.plot_con_mat(pipe_submission, X_valid, y_valid, display_labels=np.unique(full_dataframe.Gloss), xticks_rotation='horizontal', save_plot=save_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Kaggle submission\n",
    "\n",
    "Note: Currently testing is performed with GridSearchCV best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a submission using the test set data and write the submission file using the provided code\n",
    "\n",
    "all_test_files = sorted(glob(pjoin(TEST_DIR, '*.npy')))\n",
    "\n",
    "test_samples = []\n",
    "for numpy_file in all_test_files:\n",
    "    sample = np.load(numpy_file)\n",
    "    test_samples.append(sample)\n",
    "\n",
    "samples_centered_4D_array_test = feat_prepro.interpolate_allsamples(test_samples, interpolated_total_frames=interpolated_total_frames, x_resolution=1.0, y_resolution=1.0)\n",
    "\n",
    "print(\"Interpolated test data shape\",samples_centered_4D_array_test.shape)\n",
    "print(\"\\nExtracting features for testing data\")\n",
    "X_test = feat_extract.main_feature_extractor(array_4D_data=samples_centered_4D_array_test, face=face_flag, body=body_flag, hands=hand_flag, physics=physics_flag, trajectory=trajectory_flag, linear_flag=linear_flag, angular_flag=angular_flag, std_flag=std_flag, velocity_flag=velocity_flag, acceleration_flag=acceleration_flag, remove_keypoints=remove_keypoints, unwanted_keypoints=unwanted_keypoints)\n",
    "print(\"Test shape\",X_test.shape)\n",
    "print(\"NAN values:\",np.isnan(X_test).sum())\n",
    "\n",
    "test_probas = pipe_submission.predict_proba(X_test)\n",
    "fname_txt = 'main10_LightGBM_body_phy_traj'#'main1_logreg'\n",
    "H.create_submission(test_probas, '{txt}.csv'.format(txt=fname_txt))\n",
    "\n",
    "print(\"\\nKaggle submission {txt}.csv generated. Check the current directory.\".format(txt=fname_txt))\n",
    "print(\"\\n~~~~~#####      Done     #####~~~~~\\n\")\n",
    "\n",
    "timeElapsed = datetime.now() - startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Parameter grid for various estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature set 1: mean value of x-y-c over 15 frames i.e. 137*3 = 411 features\n",
    "\n",
    "Feature set 2: Removing keypoints. (body+face+hands) + mean of x-y-c + trajectory = 14+3+16 + 216 + 8 = 256 features\n",
    "\n",
    "#### Logistic Regression\n",
    "Feature set 1: param_grid = dict(clf__C=[0.25, 0.275, 0.30])\n",
    "Feature set 2: param_grid = dict(clf__C=[0.85, 0.875, 0.9, 0.925, 0.95]) \n",
    "\n",
    "#### Support Vector Classifier 4 models\n",
    "1. With RBF kernel and OneVsOne decision function: param_grid = dict(clf__C=[5.75, 6.0, 6.25, 6.5, 6.75, 7.0], clf__gamma=['scale', 'auto'])\n",
    "2. With RBF kernel and OneVsRest decision function: param_grid = dict(clf__C=[5.75, 6.0, 6.25, 6.5, 6.75, 7.0], clf__gamma=['scale', 'auto'])\n",
    "3. With Linear kernel and OneVsOne decision function: param_grid = dict(clf__C=[0.075, 0.0775, 0.08, 0.0825, 0.085], clf__gamma=['scale', 'auto'])\n",
    "4. With Linear kernel and OneVsRest decision function: param_grid = dict(clf__C=[0.075, 0.0775, 0.08, 0.0825, 0.085], clf__gamma=['scale', 'auto'])\n",
    "\n",
    "#### Random Forest\n",
    "Note: Please select 1 or 2 or 3 parameter at a time to reduce the computation time.\n",
    "\n",
    "param_grid = dict(clf__n_estimators=[128, 130, 132, 134, 136], clf__max_depth=[12, 14, 16, 18], clf__max_features=['log', 'auto', None], clf__max_leaf_nodes=[130, 140, 150, 160, None], clf__bootstrap=[True, False], clf__criterion=['gini', 'entropy'], clf__max_samples=[0.2, 0.4, 0.6, 0.8, None])\n",
    "\n",
    "\n",
    "#### Light Gradient Bossting Machine - LightGBM\n",
    "Note: Please select 1 or 2 or 3 parameter at a time to reduce the computation time.\n",
    "\n",
    "Feature set 1: param_grid = dict(clf__num_leaves=[11, 13, 15, 17, 19], max_depth=[11, 12, 13, 14, 15, 16], learning_rate=[0.5, 0.525, 0.55, 0.575, 0.6, 0.625, 0.65], n_estimators=[101, 102, 103, 104, 105], clf__importance_type=['split', 'gain'])\n",
    "\n",
    "Feature set 2: param_grid = dict(clf__num_leaves=[16, 18, 20], max_depth=[4, 5, 6, 7], learning_rate=[0.5, 0.525, 0.55], n_estimators=[140, 150, 160, 170, 180])\n",
    "\n",
    "\n",
    "#### Gaussian Naive Bayes\n",
    "param_grid = dict(clf__var_smoothing=[90, 95, 100, 105, 110, 115, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
